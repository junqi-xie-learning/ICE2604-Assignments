{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import json\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "            Chrome/80.0.3987.162 Safari/537.36 Edg/80.0.361.109\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aff(url, headers, id, page):\n",
    "    data = {\n",
    "        \"limit\": 30,\n",
    "        \"timeout\": 3000,\n",
    "        \"filterTags\": [],\n",
    "        \"tagId\": id,\n",
    "        \"fromLemma\": \"true\",\n",
    "        \"contentLength\": 38,\n",
    "        \"page\": page }\n",
    "    response = requests.post(url, headers=headers, data=data)\n",
    "    jdata = json.loads(response.content)\n",
    "    urls = [(i['lemmaTitle'], i['lemmaUrl']) for i in jdata['lemmaList']]\n",
    "    total_page = jdata['totalPage'] # the range of crawling\n",
    "    return (urls, total_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 76604\n",
    "total_page = get_aff('https://baike.baidu.com/wikitag/api/getlemmas', headers, id, 0)[1]\n",
    "urls = []\n",
    "for i in range(total_page):\n",
    "    sleep(0.05) # Avoid crawling too fast\n",
    "    new_urls = get_aff('https://baike.baidu.com/wikitag/api/getlemmas', headers, id, i)[0]\n",
    "    for url in new_urls:\n",
    "        # Filter out the duplicated pages\n",
    "        if url[1] not in urls:\n",
    "            urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = { }\n",
    "for url in urls:\n",
    "    sleep(0.05)\n",
    "    response = requests.get(url[1], headers=headers)\n",
    "    response.ecoding = 'urf-8'\n",
    "    tree = etree.HTML(response.text)\n",
    "    entry = url[0]\n",
    "    summary = ''.join(tree.xpath('//div[@class=\"lemma-summary\"]//text()')).replace('\\n', '').replace('\t', '')\n",
    "    print(entry)\n",
    "    \n",
    "    if entry in info:\n",
    "        n = 2\n",
    "        while \"{} ({})\".format(entry, n) in info:\n",
    "            n += 1\n",
    "        entry = \"{} ({})\".format(entry, n)\n",
    "    info[entry] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"baike_data.json\", \"w\", encoding = \"utf8\") as f:\n",
    "    json.dump(info, f, ensure_ascii=False)"
   ]
  }
 ]
}